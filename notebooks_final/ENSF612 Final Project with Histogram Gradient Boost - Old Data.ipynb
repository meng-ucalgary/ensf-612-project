{"cells":[{"cell_type":"markdown","source":["#README Classifier\n\n1.\tG. A. A. Prana, C. Treude, F. Thung, T. Atapattu, and D. Lo, “Categorizing the Content of GitHub README Files - Empirical Software Engineering,” SpringerLink, 12-Oct-2018. [Online]. Available: https://link.springer.com/article/10.1007/s10664-018-9660-3. [Accessed: 11-Dec-2021].\n2.\tGprana. “Gprana/READMEClassifier.” GitHub, https://github.com/gprana/READMEClassifier.\n\n\n\n##Install modules and make imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50eed502-073e-4571-bf8c-d79d829f481a"}}},{"cell_type":"code","source":["%sh\npip install nltk\npip install click\npip install html2text\npip install lxml\npip install markdown2\npip install numpy\npip install pandas\npip install python-dateutil\npip install pytz\npip install regex\npip install scikit-learn\npip install scipy\npip install six\npip install sklearn\npip install soupsieve\npip install threadpoolctl\npip install tqdm\npip install bs4\npip install beautifulsoup4\npip install --upgrade pip\npython -m nltk.downloader all"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06340cdf-e2e5-485f-a0e6-0490f203b99b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.5)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.62.3)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.0.3)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.11.10)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (8.0.3)\nRequirement already satisfied: html2text in /databricks/python3/lib/python3.8/site-packages (2020.1.16)\nRequirement already satisfied: lxml in /databricks/python3/lib/python3.8/site-packages (4.6.4)\nRequirement already satisfied: markdown2 in /databricks/python3/lib/python3.8/site-packages (2.4.2)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (1.19.2)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (1.2.4)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas) (2020.5)\nRequirement already satisfied: numpy>=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas) (1.19.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil) (1.15.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (2020.5)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (2021.11.10)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.8/site-packages (0.24.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\nRequirement already satisfied: numpy>=1.13.3 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.19.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (1.6.2)\nRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from scipy) (1.19.2)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (1.15.0)\nRequirement already satisfied: sklearn in /databricks/python3/lib/python3.8/site-packages (0.0)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.8/site-packages (from sklearn) (0.24.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\nRequirement already satisfied: numpy>=1.13.3 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\nRequirement already satisfied: soupsieve in /databricks/python3/lib/python3.8/site-packages (2.3.1)\nRequirement already satisfied: threadpoolctl in /databricks/python3/lib/python3.8/site-packages (2.1.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (4.62.3)\nRequirement already satisfied: bs4 in /databricks/python3/lib/python3.8/site-packages (0.0.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.8/site-packages (from bs4) (4.10.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.3.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.8/site-packages (4.10.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.8/site-packages (from beautifulsoup4) (2.3.1)\nRequirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (21.3.1)\n/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /root/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /root/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to /root/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /root/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /root/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /root/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to /root/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /root/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to /root/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /root/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /root/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /root/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /root/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to /root/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n[nltk_data]    |   Package wordnet31 is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /root/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: nltk in /databricks/python3/lib/python3.8/site-packages (3.6.5)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk) (4.62.3)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk) (8.0.3)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.8/site-packages (from nltk) (2021.11.10)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (8.0.3)\nRequirement already satisfied: html2text in /databricks/python3/lib/python3.8/site-packages (2020.1.16)\nRequirement already satisfied: lxml in /databricks/python3/lib/python3.8/site-packages (4.6.4)\nRequirement already satisfied: markdown2 in /databricks/python3/lib/python3.8/site-packages (2.4.2)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (1.19.2)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (1.2.4)\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas) (2020.5)\nRequirement already satisfied: numpy>=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas) (1.19.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.8/site-packages (2.8.1)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.8/site-packages (from python-dateutil) (1.15.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (2020.5)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (2021.11.10)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.8/site-packages (0.24.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\nRequirement already satisfied: numpy>=1.13.3 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (1.19.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (1.6.2)\nRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from scipy) (1.19.2)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (1.15.0)\nRequirement already satisfied: sklearn in /databricks/python3/lib/python3.8/site-packages (0.0)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.8/site-packages (from sklearn) (0.24.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\nRequirement already satisfied: numpy>=1.13.3 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\nRequirement already satisfied: soupsieve in /databricks/python3/lib/python3.8/site-packages (2.3.1)\nRequirement already satisfied: threadpoolctl in /databricks/python3/lib/python3.8/site-packages (2.1.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (4.62.3)\nRequirement already satisfied: bs4 in /databricks/python3/lib/python3.8/site-packages (0.0.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.8/site-packages (from bs4) (4.10.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.3.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.8/site-packages (4.10.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.8/site-packages (from beautifulsoup4) (2.3.1)\nRequirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (21.3.1)\n/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /root/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /root/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package brown to /root/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /root/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package floresta to /root/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /root/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /root/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /root/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /root/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to /root/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pil to /root/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package propbank to /root/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package ptb to /root/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package qc to /root/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to /root/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rte to /root/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package semcor to /root/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to /root/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to /root/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package state_union to /root/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package timit to /root/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /root/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to /root/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n[nltk_data]    |   Package wordnet31 is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | Downloading package rslp to /root/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# standard modules\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport re\n\n# feature engineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import LabelBinarizer\n\n# classifiers\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n\n# measures\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report\n\n# other\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom sklearn.utils import resample\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.base import BaseEstimator, clone\n\nnltk.download('words')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4467c648-3c5c-4d02-8577-612c356b606c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\nOut[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\nOut[2]: True"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["##Preparation\n\n###Code for deriving heuristic features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9b15eba-9e56-4b78-811f-82fda32f80a6"}}},{"cell_type":"code","source":["class Heuristic_Feature:\n  \"\"\"\n  Class for deriving heuristic features from input text\n\n  Naming convention:\n    heur_<x>_<y>_<number>\n      <x> = c if meant for use in content text, h if meant for use with header\n      <y> = k if uses keyword matching, s if uses text statistics (e.g. word count), c if combination\n  \"\"\"\n\n  def heur_c_k_001(self, input_text):\n    if ('report bugs' in input_text.lower()) or ('reporting bugs' in input_text.lower()):\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_002(self, input_text):\n    if ' is a ' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_003(self, input_text):\n    if '@abstr_code_section' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_004(self, input_text):\n    if 'attempts to' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_005(self, input_text):\n    if 'inspired by' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_006(self, input_text):\n    if 'install ' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_007(self, input_text):\n    if 'reasons' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  # Do not use lower() because we want to capture \"Added\" at beginning of sentence\n  def heur_c_k_008(self, input_text):\n    if 'Added ' in input_text:\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_009(self, input_text):\n    if 'copyright' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_010(self, input_text):\n    if '@abstr_mailto' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_011(self, input_text):\n    if 'you can ' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  # Check if the text comprises solely of @abstr_code_section\n  def heur_c_k_012(self, input_text):\n    if '@abstr_code_section' == input_text.lower().strip():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_013(self, input_text):\n    if 'About' in input_text:\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_014(self, input_text):\n    if 'be sure to' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_015(self, input_text):\n    if 'Download' in input_text:\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_016(self, input_text):\n    if 'overview' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_017(self, input_text):\n    if 'get started' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_018(self, input_text):\n    if 'reasons' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_019(self, input_text):\n    if 'dependenc' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_020(self, input_text):\n    if 'rerun' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_021(self, input_text):\n    if 'you''ll be able' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_022(self, input_text):\n    if 'you must' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_c_k_023(self, input_text):\n    if 'previous version' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_001(self, input_text):\n    if 'configur' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_002(self, input_text):\n    if 'what' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_003(self, input_text):\n    if 'why' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_004(self, input_text):\n    if 'approach' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_005(self, input_text):\n    if 'bugs' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_006(self, input_text):\n    if 'contrib' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_007(self, input_text):\n    if 'credit' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_008(self, input_text):\n    if 'feature' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_009(self, input_text):\n    if 'install' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_010(self, input_text):\n    if 'intro' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_011(self, input_text):\n    if 'licen' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_012(self, input_text):\n    if 'objective' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_013(self, input_text):\n    if 'request' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_014(self, input_text):\n    if 'requirement' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_015(self, input_text):\n    if 'resource' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_016(self, input_text):\n    if 'setting' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_017(self, input_text):\n    if 'setup' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_018(self, input_text):\n    if 'started' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_019(self, input_text):\n    if 'usage' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_020(self, input_text):\n    if 'version' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_021(self, input_text):\n    if 'welcome' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_022(self, input_text):\n    if 'what is' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_023(self, input_text):\n    if 'overview' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_024(self, input_text):\n    if 'basic' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_025(self, input_text):\n    if 'roadmap' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_026(self, input_text):\n    if 'todo' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_027(self, input_text):\n    if 'example' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_028(self, input_text):\n    if 'about' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  def heur_h_k_029(self, input_text):\n    if 'reference' in input_text.lower():\n      return 1\n    else:\n      return 0\n\n  # Return 1 if string is single non-English word\n  # WARNING: For speed, instantiate the word set outside and pass it as parameter,\n  # so that instantiation will be done once for each program run\n  # instead of once for each row this heuristic is applied to\n  def heur_h_c_001(self, input_text, words=None):\n    lcase_input_text = input_text.lower()\n    s = lcase_input_text.split(' ')\n    if len(s) != 1:\n      return 0\n    else:\n      if words is None:\n        words = set(nltk.corpus.words.words())\n      if s[0] in words:\n        return 0\n      else:\n        return 1\n    return 0\n\n  # Returns 1 if a word in heading text is in repo name, or the other way around\n  def heur_h_c_002(self, heading_text, repo_url):\n    heading_words = heading_text.lower().split(' ')\n    repo_name = re.sub(r\"http(s)*:\\/\\/www\\.github\\.com\\/.+\\/\", '', repo_url)\n    repo_name = repo_name.replace('.', ' ').replace('-', ' ')\n    repo_name_words = repo_name.lower().split(' ')\n    match = [val for val in heading_words if val in repo_name_words]\n    if len(match) > 0:\n      return 1\n    else:\n      return 0\n\n  # See whether text comprises entirely of ASCII characters\n  def heur_c_s_001(self, input_text):\n    try:\n      input_text.encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n      return 0\n    else:\n      return 1\n\n  # Generate DataFrame of derived features using DataFrames of some initial features\n  def derive_features_using_heuristics(self, url_corpus, heading_text_corpus, content_corpus):\n    derived_features = pd.DataFrame()\n    derived_features['heur_c_k_001'] = [self.heur_c_k_001(x) for x in content_corpus]\n    derived_features['heur_c_k_002'] = [self.heur_c_k_002(x) for x in content_corpus]\n    derived_features['heur_c_k_003'] = [self.heur_c_k_003(x) for x in content_corpus]\n    derived_features['heur_c_k_004'] = [self.heur_c_k_004(x) for x in content_corpus]\n    derived_features['heur_c_k_005'] = [self.heur_c_k_005(x) for x in content_corpus]\n    derived_features['heur_c_k_006'] = [self.heur_c_k_006(x) for x in content_corpus]\n    derived_features['heur_c_k_007'] = [self.heur_c_k_007(x) for x in content_corpus]\n    derived_features['heur_h_k_001'] = [self.heur_h_k_001(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_002'] = [self.heur_h_k_002(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_003'] = [self.heur_h_k_003(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_004'] = [self.heur_h_k_004(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_005'] = [self.heur_h_k_005(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_006'] = [self.heur_h_k_006(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_007'] = [self.heur_h_k_007(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_008'] = [self.heur_h_k_008(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_009'] = [self.heur_h_k_009(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_010'] = [self.heur_h_k_010(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_011'] = [self.heur_h_k_011(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_012'] = [self.heur_h_k_012(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_013'] = [self.heur_h_k_013(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_014'] = [self.heur_h_k_014(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_015'] = [self.heur_h_k_015(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_016'] = [self.heur_h_k_016(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_017'] = [self.heur_h_k_017(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_018'] = [self.heur_h_k_018(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_019'] = [self.heur_h_k_019(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_020'] = [self.heur_h_k_020(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_021'] = [self.heur_h_k_021(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_022'] = [self.heur_h_k_022(x) for x in heading_text_corpus]\n    derived_features['heur_c_s_001'] = [self.heur_c_s_001(x) for x in heading_text_corpus]\n\n    # Batch 02\n    derived_features['heur_c_k_008'] = [self.heur_c_k_008(x) for x in content_corpus]\n    derived_features['heur_c_k_009'] = [self.heur_c_k_009(x) for x in content_corpus]\n    derived_features['heur_c_k_010'] = [self.heur_c_k_010(x) for x in content_corpus]\n    derived_features['heur_c_k_011'] = [self.heur_c_k_011(x) for x in content_corpus]\n    derived_features['heur_c_k_012'] = [self.heur_c_k_012(x) for x in content_corpus]\n    derived_features['heur_c_k_013'] = [self.heur_c_k_013(x) for x in content_corpus]\n    derived_features['heur_h_k_023'] = [self.heur_h_k_023(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_024'] = [self.heur_h_k_024(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_025'] = [self.heur_h_k_025(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_026'] = [self.heur_h_k_026(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_027'] = [self.heur_h_k_027(x) for x in heading_text_corpus]\n    words = set(nltk.corpus.words.words())\n    derived_features['heur_h_c_001'] = [self.heur_h_c_001(x, words) for x in heading_text_corpus]\n    derived_features['heur_h_c_002'] = [self.heur_h_c_002(x, y) for x, y in zip(heading_text_corpus, url_corpus)]\n\n    # Batch 03\n    derived_features['heur_c_k_014'] = [self.heur_c_k_014(x) for x in content_corpus]\n    derived_features['heur_c_k_015'] = [self.heur_c_k_015(x) for x in content_corpus]\n    derived_features['heur_c_k_016'] = [self.heur_c_k_016(x) for x in content_corpus]\n    derived_features['heur_c_k_017'] = [self.heur_c_k_017(x) for x in content_corpus]\n    derived_features['heur_c_k_018'] = [self.heur_c_k_018(x) for x in content_corpus]\n    derived_features['heur_c_k_019'] = [self.heur_c_k_019(x) for x in content_corpus]\n    derived_features['heur_c_k_020'] = [self.heur_c_k_020(x) for x in content_corpus]\n    derived_features['heur_c_k_021'] = [self.heur_c_k_021(x) for x in content_corpus]\n    derived_features['heur_c_k_022'] = [self.heur_c_k_022(x) for x in content_corpus]\n    derived_features['heur_c_k_023'] = [self.heur_c_k_023(x) for x in content_corpus]\n    derived_features['heur_h_k_028'] = [self.heur_h_k_028(x) for x in heading_text_corpus]\n    derived_features['heur_h_k_029'] = [self.heur_h_k_029(x) for x in heading_text_corpus]\n\n    return derived_features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab8d4c96-9b4b-45ec-a657-3e148e908755"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Code for OneVsRestClassifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac154597-4c3d-4779-b566-b0348900d3d5"}}},{"cell_type":"code","source":["class _ConstantPredictor(BaseEstimator):\n  def fit(self, X, y):\n    self.y_ = y\n    return self\n\n  def predict(self, X):\n    check_is_fitted(self, 'y_')\n    return np.repeat(self.y_, X.shape[0])\n\n  def decision_function(self, X):\n    check_is_fitted(self, 'y_')\n    return np.repeat(self.y_, X.shape[0])\n\n  def predict_proba(self, X):\n    check_is_fitted(self, 'y_')\n    return np.repeat([np.hstack([1 - self.y_, self.y_])], X.shape[0], axis=0)\n\n\ndef _fit_binary(estimator, X, y, classes=None):\n  \"\"\"\n  Fit a single binary estimator\n  \"\"\"\n  unique_y = np.unique(y)\n\n  if len(unique_y) == 1:\n    if classes is not None:\n      if y[0] == -1:\n        c = 0\n      else:\n        c = y[0]\n    estimator = _ConstantPredictor().fit(X, unique_y)\n\n  else:\n    estimator = clone(estimator)\n    estimator.fit(X, y)\n\n  return estimator\n\n\nclass OneVsRestClassifierBalance(OneVsRestClassifier):\n  def fit(self, X, y):\n    self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n    Y = self.label_binarizer_.fit_transform(y)\n    Y = Y.tocsc()\n    self.classes_ = self.label_binarizer_.classes_\n    totalIns = Y.shape[0]\n    XBal = []\n    YBal = []\n\n    for i in range(len(self.label_binarizer_.classes_)):\n      if len(y.shape) > 1:\n        # Matrix\n        curIdxs = Y[:, i].nonzero()[0]\n      else:\n        curIdxs = Y.nonzero()[0]\n\n      baseX = X[curIdxs, :]\n\n      if len(y.shape) > 1:\n        # Matrix\n        baseY = y[curIdxs, :]\n      else:\n        # array, e.g. due to testing classifier performance for single label prediction\n        baseY = y[curIdxs]\n\n      tempX = X\n      tempY = y\n      imbalancedIns = baseX.shape[0]\n      numDup = totalIns/imbalancedIns - 1\n\n      for j in range(int(numDup)):\n        tempX = np.vstack((tempX, baseX))\n\n        if len(y.shape) > 1:\n          tempY = np.vstack((tempY, baseY))\n\n        else:\n          tempY = np.concatenate((tempY, baseY))\n\n      numAdd = totalIns % imbalancedIns\n      tempX = np.vstack(\n        (tempX, resample(baseX, n_samples=numAdd, random_state=0)))\n\n      if len(y.shape) > 1:\n        tempY = np.vstack(\n          (tempY, resample(baseY, n_samples=numAdd, random_state=0)))\n\n      else:\n        tempY = np.concatenate(\n          (tempY, resample(baseY, n_samples=numAdd, random_state=0)))\n\n      XBal.append(tempX)\n\n      if len(y.shape) > 1:\n        YBal.append(tempY[:, i])\n\n      else:\n        YBal.append(tempY)\n\n    self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(self.estimator, XBal[i], YBal[i], classes=[\"not %s\" % self.label_binarizer_.classes_[i], self.label_binarizer_.classes_[i]]) for i in range(len(YBal)))\n    return self"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b1214e-989a-415b-8dcd-98c242c78bf2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Loading data\nLoading the old dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dac056dc-caa2-4db4-8bcc-8d5032d7617c"}}},{"cell_type":"code","source":["def read_CSV_to_DF(filepath):\n  \"\"\"\n  Reads a csv file into a spark dataframe\n  and returns Pandas dataframe\n  \"\"\"\n  df = (\n        sqlContext\n        .read.format(\"csv\")\n        .option(\"header\", \"true\")\n    \n        .load(filepath)\n          )\n  \n  return df.toPandas()\n \n\n# importing files from DBFS\ndf = read_CSV_to_DF('/FileStore/FinalProject/raw_data.csv')\n \n# cast the some columns to int\ndf['file_id'] = df['file_id'].astype(int)\ndf['section_id'] = df['section_id'].astype(int)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83cf60bd-7c7e-41f1-99e9-610e89ce3990"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Feature Engineering\n###Extracting statistical features using TF-IDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fbe94dd-d202-4d03-acaa-c79fcd942cf3"}}},{"cell_type":"code","source":["heading_plus_content_corpus = df['abstracted_heading_plus_content']\n\ntfidf = TfidfVectorizer(ngram_range=(1,1), analyzer='word', stop_words='english')\ntfidfX = tfidf.fit_transform(heading_plus_content_corpus)\nfeatures_tfidf = pd.DataFrame(tfidfX.todense())\nfeatures_tfidf.columns = tfidf.get_feature_names()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbac05a-45b4-479c-99ab-6e6dce42b94a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Extracting heuristic features using Heuristic_Feature class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa2b8de1-3122-4016-89a3-4dee6113f436"}}},{"cell_type":"code","source":["content_corpus = df['content_text_w_o_tags']\nheading_text_corpus = df['heading_text']\nurl_corpus = df['url']\nheuristic = Heuristic_Feature()\nderived_features = heuristic.derive_features_using_heuristics(url_corpus, heading_text_corpus, content_corpus)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"debb8237-745e-4688-90de-33383f1e3d91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Combining features together"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fceb9ea-f604-4521-b7a9-1498d53af9c3"}}},{"cell_type":"code","source":["features_combined = pd.concat([features_tfidf, derived_features], axis=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea1cd127-3081-4bb4-a989-024998264347"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###Encode the target vector"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43027d06-8896-4b09-bffe-c57c137db3dc"}}},{"cell_type":"code","source":["# Transform the target vector y\nlabel_set = ['-','1','3','4','5','6','7','8']             # Class '2' has been merged into class '1'\nlabels = [str(x).split(',') for x in df['section_code']]\n\nmlb = MultiLabelBinarizer(classes=label_set)\nlabels_matrix = mlb.fit_transform(labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73f99a3e-20d7-43b0-ba5c-5c0f42205b62"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/python/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:860: UserWarning: unknown class(es) ['                [\"\"[a-f @abstr_number - @abstr_number ]+\"\"', '         \"\"mail\"\" : \"\"sabre+ @abstr_number @tut.by\"\"         \"\"role\"\" : \"\"Store Owner\"\"       }        -- response --       Set-Cookie:  SESSe @abstr_number a @abstr_number a @abstr_number c @abstr_number a @abstr_number c @abstr_number b @abstr_number cb @abstr_number f @abstr_number =E @abstr_number juSPMd @abstr_number fUOg @abstr_number j_esS @abstr_number G @abstr_number MwU @abstr_number jodrDpBjl @abstr_number KHfli @abstr_number ; expires=Sat', \"         forro = require('forro')\", '     \"\"express\"\": \"\"^ @abstr_number . @abstr_number . @abstr_number \"\"', '     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', '   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', '   * an @abstr_hyperlink -based completion engine for C#', ' \"\"alternate names\"\"', ' \"\"hello there\"\". The dialog will take the incoming message and find the Best Match in the list of strings', ' \"\"key @abstr_number \"\": { \"\"dataA\"\": \"\"valueA\"\"', ' \"\"trouble\"\"', ' \"\"website\"\": \"\"\"\"', ' \"\"Ͷ�����\"\"', ' --debug DEBUG           Enable debug mode   The application also takes the regular parameters for APIC address', ' --input INPUT           Input file       -d DEBUG', ' --login LOGIN           APIC login ID.       -p PASSWORD', ' --outfile FILE       Filename and base module name of the generated parser        -t', ' --password PASSWORD  APIC login password.       -i INPUT', ' --url URL                 APIC IP address.       -l LOGIN', ' .offsetParent()', ' @abstr_code_section    * See below for other simulation configuration options that you can set from the command line or from a configuration file    * Once your simulation has completed', ' @abstr_number ', \" @abstr_number )) @abstr_code_section evtList = [ { date: new Date(' @abstr_number '\", ' BUT NOT LIMITED TO', ' Debian @abstr_number and CentOS @abstr_number / @abstr_number     \"', ' EXPRESS OR IMPLIED', ' Estates', ' I take this methodology to the world of node.js. This time', ' Inc. @abstr_number Montague ST STE @abstr_number BROOKLYN', ' MB) bar.SetUnits(pb.U_BYTES)  // and start bar.Start() @abstr_code_section go // create and start bar bar := pb.New(myDataLen).SetUnits(pb.U_BYTES) bar.Start()  // my io.Reader r := myReader  // my io.Writer w := myWriter  // create proxy reader reader := bar.NewProxyReader(r)  // and copy from pb reader io.Copy(w', ' Main Estate   The UDP channel is a “Road\"\"   * The members of a Road are “Estates” (as in real estate lots that front the road)   * Each Estate has a unique UDP Host Port address “ha” ', ' NSErrorFailingURLKey=https://api.leancloud.cn/ @abstr_number . @abstr_number /batch/save', ' TLS is also problematic as a security system both from performance and vulnerabilty aspects.  Elliptic Curve Cryptography', ' Version @abstr_number . @abstr_number (the \"\"License\"\");     > SystemBarTint    * Link: @abstr_hyperlink    * License: Licensed under the Apache License', ' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', \" ['sass'\", \" \\\\ 'notes' : @abstr_number \", ' a light-weight', ' a micro-threaded multi-process application has instead one micro-thread per logical concurrent function and the total number of micro-threads is distributed amoungst a minimal number of processes', ' a number of software packages out there already that will read these files', ' a simple micro-threaded application is limited to one CPU core. To enable full utilization of all CPU cores', ' add some ads', ' adding persistence', ' an oven and a microwave (hot plates are not electric but gas powered).   8. Sub_metering_ @abstr_number : energy sub-metering No. @abstr_number (in watt-hour of active energy). It corresponds to the laundry room', ' and \"\"camembert\"\".  For example:  @abstr_code_section  If', ' and by adding the necessary method implementations to all classes which implement that interface.  Static methods on interfaces are backported by moving the static methods to a companion class (interface name + \"\"$\"\")', ' and cpu capacity become critical', ' and distribution of messages between publishers and subscribers via queues.    One of the advantages of a message queuing service for many applications is that the service hides behind an API', ' and interprocess communications while providing much higher total performance.  Because all the cooperative micro-threads run in one process', \" and it should take one argument (it doesn't actually matter what the argument is called\", ' and it will look for the data                                   in the file passed in the property \"\"csvReportFile\"\". If you use this property', ' and parsing all the source code of all drivers is long and unpractical.   * Some implicitly declared macros (like __KERNEL__) are needed for the code to be parsed correctly. Without them lots of functions and data structures will appear as undeclared.   * A lot of the code (power management', \" and probably also needs to have some of it removed (or migrated to 'magic.verbose' or something)\", \" and start dumping checkpoints into the folder specified by checkpoint_path (default = current folder). You also have to point the train script to the VGGNet protos (see the options inside train.lua).  If you'd like to evaluate BLEU/METEOR/CIDEr scores during training in addition to validation cross entropy loss\", ' and the associated demands on memory', ' and the language automatically does the needed cloning. For ref types', ' and then run:  @abstr_code_section  And then open a browser to http://localhost: @abstr_number \"', ' and this is the actual SpriteKit class. If all you want is some sample code for SpriteKit', ' and understand AUV', ' and writing it feels like writing HTML. A simple transform is included with React that allows converting JSX into native JavaScript for browsers to digest.  \"', ' ant] @abstr_number . Clone repository: git clone https://github.com/mitro-co/mitro @abstr_number . Install browser extension dependencies:        @abstr_code_section   @abstr_number . Run the regression tests to ensure your source tree works:        @abstr_code_section   @abstr_number . Look for \"\"SUCCESS\"\" on the last line. @abstr_number . Build the browser extension:        @abstr_code_section   @abstr_number . To build Firefox', ' are often based on a messaging or event bus that allows the various distributed components to communicate asynchronously with each other. Typically the messaging bus is some form of messaging queue service such as AMQP or ZeroMQ. The message bus supports what is commonly referred to as a publish/subscribe methodology for information exchange.  While there are many advantages to a full featured message queuing service', ' as well as parses any existing _credentials.py_ file stored in the same directory. In that case', ' because most MQ services are based on TCP/IP they tend to also use HTTP and therefore TLS/SSL for secure communications. While using HTTP provides easy integration with web based systems', ' because part of implementing OAuth requires the user to take action on the provider (Foursquare', ' both methods can be combined.  \"', ' but WITHOUT ANY WARRANTY; including but not limited to', ' but it must also have been explicitly published using the \"\"Share\"\" button in the top right corner of the Google Spreadsheets GUI.  Generally', ' but there may be cases where you\\'d like to change this. Simply set the \"\"amd\"\" option:  @abstr_code_section  Or', ' but unfortunately the formatting of these errors don\\'t make looking for it any easier. We hope to improve that in other ways (see @abstr_hyperlink )  \"', ' but you can also directly access the raw data.  Now go ahead and @abstr_hyperlink ', ' c_height);     template->code = codeNumber;     template->size = codeSize;     CRCodeImageTemplateStorageAddNewTemplate(codeImageTemplateStorage', ' check out @abstr_hyperlink .  \"', ' commas delimit coordinates; semicolons delimit positions| |a_m_kon |double | @abstr_number |s^(- @abstr_number ) |active motor on rate| |a_m_koff |double | @abstr_number |s^(- @abstr_number ) |active motor off rate| |a_m_kend |double | @abstr_number |s^(- @abstr_number ) |active motor off rate at filament end| |a_motor_stiffness |double | @abstr_number |pN/um |active motor spring stiffness| |a_motor_length |double | @abstr_number . @abstr_number |um |length of motor| |a_m_stall |double | @abstr_number |pN |stall force of motors| |a_m_break |double | @abstr_number |pN |rupture force of motors| |a_m_bind |double | @abstr_number . @abstr_number |pN_um |binding energy| |a_motor_v |double | @abstr_number |um/s |velocity along filaments towards barbed end when attached| |motor_intersect_flag |boolean|false | |if true', ' context switching', ' create a directory ext_lib within your project and populate it with symlinks to your libraries. Then set up the .tern-project something like this:  @abstr_code_section  Then', ' directing it to said directory. Then the app will be launchable from the app screen and the extensions screen.  To distribute the app', ' do \"\"Project -> Open/Import Project...\"\" and select the root Makefile of your Linux kernel. When prompted for the type of the project', ' each slide will be defined by default with an element containing the slide class: @abstr_code_section You can see a fully working example of the HTML structure in the [demoPage.html` file](https://github.com/alvarotrigo/fullPage.js/blob/master/examples/demoPage.html).  \"', ' encryption and the CurveCP handshake for secure bootstrap.  The queue management and micro-threaded application support is provided by Ioflo. RAET is a complementary project to Ioflo in that RAET enables multiple Ioflo applications to work together over a network as part of a distributed application.  The primary use case and motivating problem that resulted in the development of RAET was the need to enable SaltStack to scale better. SaltStack is a remote execution and configuration management platform written in Python. SaltStack uses ZeroMQ ( @abstr_number MQ) as its message bus or message queuing service. ZeroMQ is based on TCP/IP so suffers from the aforementioned latency and non-asynchronicity issues of TCP/IP based architectures. Moreover because ZeroMQ integrates queue management and transport in a monolithic way with special \"\"sockets\"\"', ' etc. More information and instructions for subscribing are at @abstr_hyperlink .  \"', ' evtList){}  Callback function called when user click in a day', ' execute custom python script', ' first columns and then rows are binded to a single tidy dataset.  With the help of dlpyr package data are grouped by subject and activity and then averaged. The result is saved in the working directory as summary.txt  \"', ' for easiest transition from a \"\"Vanilla\"\" server to one enhanced by StarryPy', ' function (err', ' function(err', ' hereby commit to the neveragain.tech pledge. Please stand with me and hold me to it. #neveragaintech\"\" * Post \"\"I', ' homebrew', \" how do the things in parentheses _become_ true or false?  JavaScript lets us compare things. Most of these comparisons come straight from math: we can ask if something is less than something else (enter these in your console!):  @abstr_code_section  We can ask if something is greater than something else:  @abstr_code_section  We can even ask if something is less-than-or-equal-to something else:  @abstr_code_section  or greater-than-or-equal-to something:  @abstr_code_section  How do we test if something is _exactly_ equal to something else? We know that we can't just use =\", ' iOS and Windows). The intelxdk.config.additions.xml file can be used to include options that control your _packaged Cordova web app_ builds. For example', ' if we pass utter nonsense to parseInt()? Go ahead and try it in the console — something like  @abstr_code_section  What did it return? NaN? What is that?  NaN stands for \"\"not a number\"\" — pretty handy', ' including without limitation     the rights to use', ' including without limitation the rights to use', ' including without limitation the rights_ _to use', ' infact due to a logic error in previous code the cookies were not being used anyway. Now Django Select @abstr_number does not use cookies etc.   * Few more bugs fixed in heav_data.js.   * Now production code will use minimized versions of js and css files.   * Codes added in setup.py to automate the task of minimizing js and css files', ' injection and in class annotations', ' is a tuned transport protocol that adds reliability to UDP/IP without sacrificing latency and scalability. A transactioned protocol', ' is much more appropriate for providing reliablity to asynchronous event transport than a streaming protocol.  Morover', ' it can become problematic for high performant systems Furthermore', ' it evaluates the left-hand side of the colon; otherwise it evaluates the right-hand side of the colon.  Syntax:  @abstr_code_section    * Define a function ternaryTeenager that accepts age as a parameter. The body of the function should use the ternary operator to return \"\"You are a teenager\"\" if age is between @abstr_number - @abstr_number and returns \"\"You are not a teenager\"\" if the age is anything else.    Top tip : In order for the function to actually return the evaluation of the ternary operator', ' it is much better suited to many small asynchronous messages and scales better. The drawback of bare UDP/IP is that it is not reliable. What is needed', ' it now has all the functions.  Implementation inheritance is used instead of subtyping to make it easier to understand which functions operate on any given \"\"subject\"\". If you have an element and you need to use a function defined in Node', ' it should return \"\"You are a grownup\"\" Top tip : Remember', ' it will upgrade. - Likewise', ' it\\'s an important skill to have!  \"', ' java', \" jumps to the symbol's declaration. For C/C++/Objective-C\", ' just go to your project folder and run:          github_changelog_generator     * Or', ' just take forever (actually until a timeout is reached). You need to keep this in mind in order to not block your main or UI thread.  Implications of this design choice  Pros:    * A blocking API is much easier to use and understand    Cons:    * You just might accidentally block your UI thread   * You cannot issue thousands of EWS requests asynchronously simply because you cannot spawn thousands of threads in your process. You may need additional effort here    \"', ' libqs.c and the yara signatures except where noted are Copyright @abstr_number Tyler McLellan and Tylabs.  See included Mozilla Public License Version @abstr_number . @abstr_number for licensing information. \"', ' like AMQP', \" loadFeed() is asynchronous so this test wil require the use of Jasmine's beforeEach and asynchronous done() function. @abstr_number . ---Write a test that ensures when a new feed is loaded by the loadFeed function that the content actually changes. Remember\", ' make sure that the instances are being launched with a security group that allows SSH access.  \"', ' memory', ' mitro-core/README) [node', ' network', ' network) in distributed concurrent event driven applications is to use something called micro-threads. A microthread is typically an in-language feature that allows logical concurrency with no more overhead than a function call. Micro threading uses cooperative multi-tasking instead of threads and/or processes and avoids many of the complexities of resource contention', ' no more than the number of cpu cores. This optimizes the use of the cpu power while minimizes the overhead of process context switching.  An example of a framework that uses this type of micro-threaded but multi-process architecture is Erlang. Indeed', ' npm', ' on average across all the days in the dataset', ' on the other hand', ' one might ask', ' one of the best ways to manage and fine tune processor resources (cpu', ' one of the disadvantagesis the inability to manage performance at scale.  A message queuing service performs two distinct but complementary functions.    * The first is asynchronous transport of messages over the internet.   * The second is message queue management', ' or because they are tech geeks. But these data remain under-utilized both because the raw data are hard to obtain and there is a lack of statistical methods and software for processing and interpreting the data.  This assignment makes use of data from a personal activity monitoring device. This device collects data at @abstr_number minute intervals through out the day. The data consists of two months of data from an anonymous individual collected during the months of October and November', ' or both   * Added \"\"returnName\"\" setting which makes the widget return a name instead of HEX value when possible   * Removed many unnecessary features   * Removed dependency on images and made the colour picker completely CSS     \"', ' or if you are usually handling many temporary \"\"to be in a github pull request\"\" branches', ' preface it by @abstr_number . These numbers correspond to the projections in the projectionEnum declaration in elevr-player.js.  If you want to add your picture to the drop-down', ' production)', ' provides increases in security with lower performance requirements relative to over other approaches. LibSodium provides an open source Elliptic Curve Cryptographic library with support for both authentication and encryption. The CurveCP protocol is based on LibSodium and provides a handshake protocol for bootstrapping secure network exchanges of information.  Finally', ' push your changes to your clone and submit a pull request; instructions are available at @abstr_hyperlink . (In case you need them', ' ready-to-use download of the library/framework (unminified) ( @abstr_number )   * Prefer hand-coded/hand-optimized JavaScript over generated/cross-compiled code.   * Running \"\"make\"\" should work and not return an error. To run make', ' requests', ' result will be @abstr_number : @abstr_number', \" rg -uuu is similar to grep -a -r.  @abstr_code_section  (Tip: If your ignore files aren't being adhered to like you expect\", ' rgb()', ' right?  What happens', \" ripgrep defaults to recursive directory search and won't search files ignored by your .gitignore files. It also ignores hidden and binary files by default. ripgrep also implements full support for .gitignore\", ' ruby', \" run the test suite by typing learn and hitting enter. You'll see something similar to:  @abstr_image  You can see your test is currently failing\", ' run this from anywhere:      * github_changelog_generator -u github_username -p github_project     * github_changelog_generator github_username/github_project   * If you are running it against a repository on a Github Enterprise install', ' send an email to  with subject line \"\"[web-animations] ... message topic ...\"\" ( @abstr_hyperlink ).   * For issues with the polyfill', ' simply: \"\"./build.sh test\"\"  To run all the tests  > $ ./build.sh install -DallTests  \"', \" so a typical call to parseInt() looks like  @abstr_code_section  What happens if we pass a representation of a non-integer to parseInt()? Let's try it:  @abstr_code_section  If we enter the above in console\", ' so we leave it out.   * The xact() decorator will set up the connection so that a transaction _is_ started in the relevant block', ' spreadsheet) {         spreadsheet.worksheets[ @abstr_number ].cells({             range: \"\"R @abstr_number C @abstr_number :R @abstr_number C @abstr_number \"\"         }', ' storage', ' such as \"\"m @abstr_number .medium\"\". The default value of this if not specified is \"\"m @abstr_number .medium\"\". \"\"m @abstr_number .small\"\" has been deprecated in \"\"us-east- @abstr_number \"\" and \"\"m @abstr_number .medium\"\" is the smallest instance type to support both paravirtualization and hvm AMIs   * keypair_name - The name of the keypair to use to bootstrap AMIs which support it.   * monitoring - Set to \"\"true\"\" to enable detailed monitoring.   * session_token - The session token provided by STS   * private_ip_address - The private IP address to assign to an instance within a @abstr_hyperlink   * elastic_ip - Can be set to \\'true\\'', ' templates', ' tend to be unreliable under load.  Separating the function of network transport of asynchrounous event from the function of message queue management allows independant tuning at scale of each function.  Most if not all of the MQ services are based on TCP/IP for transport. TCP/IP adds significant latency to the network communications and is therefore not well suited for the asynchronous nature of distibuted event driven application communications. This is primarily due to the way TCP/IP handles connection setup and teardown as well as failed connections in order to support streams. Fundamentally TCP/IP is optomized for sending large contiguous data streams not many small aynchronous events or messages. While not a problem for small scale systems', ' thanks to @abstr_hyperlink ! Tap \"\"Hint\"\" to show hint (e.g. Move left/right/up/down); tap \"\"Auto Run\"\" to run AI automatically. Check it out in the AI branch. You can also check out @abstr_hyperlink .  Thanks to @abstr_hyperlink \\'s Javascript version that gave me (DJBen', ' that is', \" the Erlang ecosystem is somewhat limited in comparison to Python's and the language itself uses what one might describe as a very unfortunate syntax. One of the design objectives behine RAET was to leverage existing Python expertise and the richness of the Python ecosystem but still be able to develop distributed applications using a micro-threaded multi-process architectural model. The goal was to combine the best of both worlds.  RAET is designed to provide secure reliable scalable asynchronous message/event transport over the internet in a micro-threaded multi-process application framework that uses UDP for interhost communication and LibSodium for authentication\", ' the application needs to be able to run at least one process per CPU core. This requires same host inter-process communications. But unlike the conventional approach to multi-processing where there is of one process per logical concurrent function', \" the autoScrolling functionality will still work as expected. The user will also be free to scroll the site with the scroll bar and fullPage.js will fit the section in the screen when scrolling finishes.    * paddingTop: (default @abstr_number) Defines the top padding for each section with a numerical value and its measure (paddingTop: ' @abstr_number px'\", ' the client has little ability to tune the service for performance. Often MQ services become bottlenecks for the distributed application. The more complicated MQ services', ' the completion \"\"getUserAccount\"\" would be ranked higher in the list than the \"\"Fooguxa\"\" completion (both of which are subsequence matches). A word-boundary character are all capital characters', ' the complexities of queue management from the clients. The disadvantage is that at scale', ' the content of the _credentials.py_ file must follow this format:       URL=\"\"https:// @abstr_number . @abstr_number . @abstr_number . @abstr_number \"\"     LOGIN=\"\"admin\"\"     PASSWORD=\"\"Ap @abstr_number cPass @abstr_number \"\"   If the _credentials.py_ does not exist and the credentials are not supplied from the command line', ' the differences in the associated traffic characteristics can become problematic at scale.  Because UDP/IP has lower latency and is connectionless', ' the entire _phrase_) to be true; with ||', ' the identification', ' the link should say \"\" @abstr_number commits\"\".  @abstr_number . You will see a list of commits that you have made to this repository. The most recent commit is at the very top. If this represents the version of the files you want to submit', \" the name of the item you wish to remove. If the item isn't in the cart\", \" the rate limit is only up to @abstr_number requests per hour. Unauthenticated requests are associated with your IP address (not the user making requests).  If you're seeing this warning\", ' the success of the Erlang model provided support for the viability of the RAET approach. Indeed', ' the time when the signal was received', ' the timing of messages', ' then start the dev server as usual with npm start:   \"', ' then the uppercase letters in your query must match uppercase letters in the completion strings (the lowercase letters still match both). So', \" then your     cmake call will be a bit more complicated.  We'll assume you downloaded a     binary distribution of LLVM+Clang from llvm.org in step  @abstr_number  and that you     extracted the archive file to folder ~/ycm_temp/llvm_root_dir (with bin\", ' there is an option of using a specific proxy to extract video information from the site: --extractor-proxy/-y.    \"', ' there will be some results', ' there\\'s nothing you need to do! Ionic @abstr_number projects by default are setup with sass and come with all the build process enabled.  \"', ' therefore', ' this is a little more expensive', ' this mode is based on this function', ' though', ' thus resulting in changing the package provider on Mac OS X', ' tracking', ' type in \"\"ping hostname_A\"\"', ' username and password', ' users will be considered \"\"logged in\"\" if they have an access token stored in their session. So', ' utilizing PWM pins  Layer @abstr_number NUMPAD keys: This is useful for gaming because games can bind separate commands to numpad_ @abstr_number and regular @abstr_number ', ' we intercept all <a href=\"\"/path\"\">...</a> clicks and call msg.setLocation(\"\"/path\"\") for you. If you want to opt out of this', ' we know that our function should look like  @abstr_code_section  But how do we make string all caps? JavaScript has a method for that! It\\'s called toUpperCase(). We can call it on any string:  @abstr_code_section  So let\\'s try it with our shout() function:  @abstr_code_section  And run our tests again:  @abstr_code_section  @abstr_image  Hey! We got one to pass!  \"', ' we need to go over some basic math. In this lab', ' we should have: IP for computer A and B (denoted as IP_A and IP_B later on in the instruction)', \" we'll see that parseInt() forces the parsed number to be an integer — which makes sense when we think about it\", \" we're going to learn about various arithmetic operators. What's an operator\", \" we're going to practice writing functions and manipulating numbers in JavaScript. First\", ' whenever you try to make a Git commit', ' where the volume of messages', ' wherein components are distributed across the internet on multiple hosts and multiple CPU cores', ' which accepts two arguments: the value to parse and the base of the value being parsed. _Usually_ you will want to work with base @abstr_number ', \" which takes our new-found speaking ability to greet our grandmother. She's not exactly deaf\", ' which was created using simulated data :  @abstr_image  Your plot will look different from the one above because you will be using the activity monitor data. Note that the above plot was made using the lattice system but you can make the same version of the plot using any plotting system you choose.  \"', ' why not use Erlang? Unfortunately', ' will be removed in a later version)    \"', ' window', ' you can choose to only include a subset of IntentKit\\'s supported applications. Subspecs exist for each handler class.       # Only includes web browsers     pod \"\"IntentKit/Browsers\"\"   For more information on what subspecs are available', ' you can specify to set the crontab user with:  @abstr_code_section  \"', ' you may find it convenient to run watch-css automatically with npm start', ' you must specify _both_ --github-site and --github-api command line options:          github_changelog_generator --github-site=\"\"https://github.yoursite.com\"\" \\\\                                --github-api=\"\"https://github.yoursite.com/api/v @abstr_number /\"\"      This generates a changelog to the CHANGELOG.md file', ' you must:  @abstr_number ) Create a code library containing one or more functions that receive a vehicle_signal_t and update the appropriate fields of a trip_event_summary_t. * the start time is the time that the first signal of any type is received * the duration is the difference in time between the first signal (of any type) received and the last signal (of any type) received * the distance travelled is the numerical integration of the VEHICLE_SIGNAL_TYPE_VEHICLE_SPEED signal * the total energy consumed is the numerical integration of the product of VEHICLE_SIGNAL_TYPE_HV_BATTERY_VOLTAGE and VEHICLE_SIGNAL_TYPE_HV_BATTERY_CURRENT * the starting SOC is the first VEHICLE_SIGNAL_TYPE_HV_BATTERY_SOC signal received * the ending SOC is the last VEHICLE_SIGNAL_TYPE_HV_BATTERY_SOC signal received  @abstr_number ) Write (at least one) unit test that feeds your library a set of vehicle_signal_t and checks that the resulting trip_event_summary_t is correct. * The included CSV file contains data from one of our employee\\'s vehicles. Use this data to generate a set of vehicle_signal_t for your unit test(s). * A good unit test asserts a result that was derived from an independent source. Document how you determined the \"\"correct value\"\" for each field in trip_event_summary_t. * Note that VEHICLE_SIGNAL_TYPE_HV_BATTERY_CURRENT is signed. An electric vehicle generally discharges the battery (positive current) when driving', ' you say? It\\'s a symbol that _operates_ on one or more (usually two) objects — + is a good example. The + operator says \"\"add what\\'s to the left of + and what\\'s to the right of + together.\"\" Easy-peasy!  As you read through this lesson', ' you should always use the --rebase flag to git pull', \" { \\\\ 'build' : { \\\\ 'mac' : './install.py'\", ' ~/.bash_profile or ~/.zshrc):           export CHANGELOG_GITHUB_TOKEN=\"\"«your- @abstr_number -digit-github-token»\"\"   So', '\"AI  An AI is added', '\"Batching a bunch of messages using the block syntax    require \\'kafka\\'     producer = Kafka::Producer.new     producer.batch do |messages|         puts \"\"Batching a send of multiple messages..\"\"         messages << Kafka::Message.new(\"\"first message to send\"\")         messages << Kafka::Message.new(\"\"second message to send\"\")     end     * they will be sent all at once', '\"Checking Authentication  The first thing we want to do is figure out if a user has already authenticated to Foursquare in this session.  Ultimately', '\"Design Notes  ews-cpp is written in a \"\"modern C++\"\" way:    * C++ Standard Library', '\"How it works  @abstr_number . HyperDev runs the start command which passes server.js through Babel (obeying options in package.json) and then runs the resulting JavaScript. @abstr_number . The server.js defines an Express server and middleware function which passes all requests ending in \"\".js\"\" through Babel\\'s transformFile. @abstr_number . User requests the index in their browser. @abstr_number . Back-end serves index.html. @abstr_number . Browser loads SystemJS. @abstr_number . Browser requests app.js. @abstr_number . Back-end transpiles app.js using same options from step @abstr_number ', '\"Installation  Add this to your composer.json:  JSON { \"\"require\"\": { \"\"thekonz/piximgen\"\": \"\" @abstr_number . @abstr_number .*@dev\"\" } } @abstr_code_section PHP require_once \\'vendor/autoload.php\\'; @abstr_code_section PHP $image = new \\\\thekonz\\\\PixImGen(); @abstr_code_section PHP $image->setSettings([ \\'seed\\' => \\'GitHub rocks!\\' ]); @abstr_code_section PHP header(\\'content-type: image/png\\'); @abstr_code_section PHP echo $image->getImage();    * Look at your image!    @abstr_image  If you play around with the settings (especially the saturation settings)', '\"Installing  This plugin requires KDevelop @abstr_number . @abstr_number .  Just typing cmake . && make && sudo make install should be enough to have the project installed. Then in KDevelop', '\"Introduction  In this lab', '\"Motivation  Modern large scale distributed application architectures', '\"Next steps  Get your computer or device to use the VPN. Please refer to:  Configure IPsec/L @abstr_number TP VPN Clients Configure IPsec/XAuth (\"\"Cisco IPsec\"\") VPN Clients  How-To: IKEv @abstr_number VPN for Windows and Android  If you get an error when trying to connect', '\"Quick Start  @abstr_number . Install dependencies (see browser-ext/README', '\"Rebasing  For feature/topic branches', '\"Road', '\"Sending a sequence of messages    require \\'kafka\\'     producer = Kafka::Producer.new     message @abstr_number  = Kafka::Message.new(\"\"some random message content\"\")     message @abstr_number  = Kafka::Message.new(\"\"some more content\"\")     producer.send([message @abstr_number ', '\"Specifically', '\"Ternary Operator  You can think of it as a shortcut for the if-else statement.  This operator tests a condition; if the condition is truthy', '\"The config.json file  The config.json file currently only requires the key \"\"states_dir\"\" and the value \"\"states/\"\" to tell the test script where to find its states relative to the root of the git repo. The \"\"/\"\" at the end of states is important don\\'t forget it. In the example above', '\"Usage    $ python @abstr_number  aci-fabric-deploy.py --input <spreadhseet>      Optional arguments:       -u URL', '\"Usage  It\\'s really simple!    * If your git remote origin refers to your GitHub repo', '\"Using unit tests  There are various unit tests included in this package to verify if certain modules are operating properly. Simply run the \"\"test\"\" prefixed python script associated with the module you would like to test.  For example to test the DrugDatabase module', '\"We love feedback!   For feedback on the API and the specification:      * File an issue on GitHub:     * Alternatively', '\"href  As a bonus', '\"if-else Statements  You will often see an if statement used in combination with an else clause. An else clause will only get executed if the previous if statement is falsey.  Syntax:  @abstr_code_section    * Define a function teenager that accepts an age as a parameter. If the age is between @abstr_number and @abstr_number it should return \"\"You are a teenager!\"\". Otherwise', '\"parseInt()  The first such tool is the function parseInt()', '否则通过URL自动获取.  @abstr_number . 分享文章给他人'] will be ignored\n  warnings.warn('unknown class(es) {0} will be ignored'\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/python/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:860: UserWarning: unknown class(es) ['                [\"\"[a-f @abstr_number - @abstr_number ]+\"\"', '         \"\"mail\"\" : \"\"sabre+ @abstr_number @tut.by\"\"         \"\"role\"\" : \"\"Store Owner\"\"       }        -- response --       Set-Cookie:  SESSe @abstr_number a @abstr_number a @abstr_number c @abstr_number a @abstr_number c @abstr_number b @abstr_number cb @abstr_number f @abstr_number =E @abstr_number juSPMd @abstr_number fUOg @abstr_number j_esS @abstr_number G @abstr_number MwU @abstr_number jodrDpBjl @abstr_number KHfli @abstr_number ; expires=Sat', \"         forro = require('forro')\", '     \"\"express\"\": \"\"^ @abstr_number . @abstr_number . @abstr_number \"\"', '     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', '   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', '   * an @abstr_hyperlink -based completion engine for C#', ' \"\"alternate names\"\"', ' \"\"hello there\"\". The dialog will take the incoming message and find the Best Match in the list of strings', ' \"\"key @abstr_number \"\": { \"\"dataA\"\": \"\"valueA\"\"', ' \"\"trouble\"\"', ' \"\"website\"\": \"\"\"\"', ' \"\"Ͷ�����\"\"', ' --debug DEBUG           Enable debug mode   The application also takes the regular parameters for APIC address', ' --input INPUT           Input file       -d DEBUG', ' --login LOGIN           APIC login ID.       -p PASSWORD', ' --outfile FILE       Filename and base module name of the generated parser        -t', ' --password PASSWORD  APIC login password.       -i INPUT', ' --url URL                 APIC IP address.       -l LOGIN', ' .offsetParent()', ' @abstr_code_section    * See below for other simulation configuration options that you can set from the command line or from a configuration file    * Once your simulation has completed', ' @abstr_number ', \" @abstr_number )) @abstr_code_section evtList = [ { date: new Date(' @abstr_number '\", ' BUT NOT LIMITED TO', ' Debian @abstr_number and CentOS @abstr_number / @abstr_number     \"', ' EXPRESS OR IMPLIED', ' Estates', ' I take this methodology to the world of node.js. This time', ' Inc. @abstr_number Montague ST STE @abstr_number BROOKLYN', ' MB) bar.SetUnits(pb.U_BYTES)  // and start bar.Start() @abstr_code_section go // create and start bar bar := pb.New(myDataLen).SetUnits(pb.U_BYTES) bar.Start()  // my io.Reader r := myReader  // my io.Writer w := myWriter  // create proxy reader reader := bar.NewProxyReader(r)  // and copy from pb reader io.Copy(w', ' Main Estate   The UDP channel is a “Road\"\"   * The members of a Road are “Estates” (as in real estate lots that front the road)   * Each Estate has a unique UDP Host Port address “ha” ', ' NSErrorFailingURLKey=https://api.leancloud.cn/ @abstr_number . @abstr_number /batch/save', ' TLS is also problematic as a security system both from performance and vulnerabilty aspects.  Elliptic Curve Cryptography', ' Version @abstr_number . @abstr_number (the \"\"License\"\");     > SystemBarTint    * Link: @abstr_hyperlink    * License: Licensed under the Apache License', ' WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND', \" ['sass'\", \" \\\\ 'notes' : @abstr_number \", ' a light-weight', ' a micro-threaded multi-process application has instead one micro-thread per logical concurrent function and the total number of micro-threads is distributed amoungst a minimal number of processes', ' a number of software packages out there already that will read these files', ' a simple micro-threaded application is limited to one CPU core. To enable full utilization of all CPU cores', ' add some ads', ' adding persistence', ' an oven and a microwave (hot plates are not electric but gas powered).   8. Sub_metering_ @abstr_number : energy sub-metering No. @abstr_number (in watt-hour of active energy). It corresponds to the laundry room', ' and \"\"camembert\"\".  For example:  @abstr_code_section  If', ' and by adding the necessary method implementations to all classes which implement that interface.  Static methods on interfaces are backported by moving the static methods to a companion class (interface name + \"\"$\"\")', ' and cpu capacity become critical', ' and distribution of messages between publishers and subscribers via queues.    One of the advantages of a message queuing service for many applications is that the service hides behind an API', ' and interprocess communications while providing much higher total performance.  Because all the cooperative micro-threads run in one process', \" and it should take one argument (it doesn't actually matter what the argument is called\", ' and it will look for the data                                   in the file passed in the property \"\"csvReportFile\"\". If you use this property', ' and parsing all the source code of all drivers is long and unpractical.   * Some implicitly declared macros (like __KERNEL__) are needed for the code to be parsed correctly. Without them lots of functions and data structures will appear as undeclared.   * A lot of the code (power management', \" and probably also needs to have some of it removed (or migrated to 'magic.verbose' or something)\", \" and start dumping checkpoints into the folder specified by checkpoint_path (default = current folder). You also have to point the train script to the VGGNet protos (see the options inside train.lua).  If you'd like to evaluate BLEU/METEOR/CIDEr scores during training in addition to validation cross entropy loss\", ' and the associated demands on memory', ' and the language automatically does the needed cloning. For ref types', ' and then run:  @abstr_code_section  And then open a browser to http://localhost: @abstr_number \"', ' and this is the actual SpriteKit class. If all you want is some sample code for SpriteKit', ' and understand AUV', ' and writing it feels like writing HTML. A simple transform is included with React that allows converting JSX into native JavaScript for browsers to digest.  \"', ' ant] @abstr_number . Clone repository: git clone https://github.com/mitro-co/mitro @abstr_number . Install browser extension dependencies:        @abstr_code_section   @abstr_number . Run the regression tests to ensure your source tree works:        @abstr_code_section   @abstr_number . Look for \"\"SUCCESS\"\" on the last line. @abstr_number . Build the browser extension:        @abstr_code_section   @abstr_number . To build Firefox', ' are often based on a messaging or event bus that allows the various distributed components to communicate asynchronously with each other. Typically the messaging bus is some form of messaging queue service such as AMQP or ZeroMQ. The message bus supports what is commonly referred to as a publish/subscribe methodology for information exchange.  While there are many advantages to a full featured message queuing service', ' as well as parses any existing _credentials.py_ file stored in the same directory. In that case', ' because most MQ services are based on TCP/IP they tend to also use HTTP and therefore TLS/SSL for secure communications. While using HTTP provides easy integration with web based systems', ' because part of implementing OAuth requires the user to take action on the provider (Foursquare', ' both methods can be combined.  \"', ' but WITHOUT ANY WARRANTY; including but not limited to', ' but it must also have been explicitly published using the \"\"Share\"\" button in the top right corner of the Google Spreadsheets GUI.  Generally', ' but there may be cases where you\\'d like to change this. Simply set the \"\"amd\"\" option:  @abstr_code_section  Or', ' but unfortunately the formatting of these errors don\\'t make looking for it any easier. We hope to improve that in other ways (see @abstr_hyperlink )  \"', ' but you can also directly access the raw data.  Now go ahead and @abstr_hyperlink ', ' c_height);     template->code = codeNumber;     template->size = codeSize;     CRCodeImageTemplateStorageAddNewTemplate(codeImageTemplateStorage', ' check out @abstr_hyperlink .  \"', ' commas delimit coordinates; semicolons delimit positions| |a_m_kon |double | @abstr_number |s^(- @abstr_number ) |active motor on rate| |a_m_koff |double | @abstr_number |s^(- @abstr_number ) |active motor off rate| |a_m_kend |double | @abstr_number |s^(- @abstr_number ) |active motor off rate at filament end| |a_motor_stiffness |double | @abstr_number |pN/um |active motor spring stiffness| |a_motor_length |double | @abstr_number . @abstr_number |um |length of motor| |a_m_stall |double | @abstr_number |pN |stall force of motors| |a_m_break |double | @abstr_number |pN |rupture force of motors| |a_m_bind |double | @abstr_number . @abstr_number |pN_um |binding energy| |a_motor_v |double | @abstr_number |um/s |velocity along filaments towards barbed end when attached| |motor_intersect_flag |boolean|false | |if true', ' context switching', ' create a directory ext_lib within your project and populate it with symlinks to your libraries. Then set up the .tern-project something like this:  @abstr_code_section  Then', ' directing it to said directory. Then the app will be launchable from the app screen and the extensions screen.  To distribute the app', ' do \"\"Project -> Open/Import Project...\"\" and select the root Makefile of your Linux kernel. When prompted for the type of the project', ' each slide will be defined by default with an element containing the slide class: @abstr_code_section You can see a fully working example of the HTML structure in the [demoPage.html` file](https://github.com/alvarotrigo/fullPage.js/blob/master/examples/demoPage.html).  \"', ' encryption and the CurveCP handshake for secure bootstrap.  The queue management and micro-threaded application support is provided by Ioflo. RAET is a complementary project to Ioflo in that RAET enables multiple Ioflo applications to work together over a network as part of a distributed application.  The primary use case and motivating problem that resulted in the development of RAET was the need to enable SaltStack to scale better. SaltStack is a remote execution and configuration management platform written in Python. SaltStack uses ZeroMQ ( @abstr_number MQ) as its message bus or message queuing service. ZeroMQ is based on TCP/IP so suffers from the aforementioned latency and non-asynchronicity issues of TCP/IP based architectures. Moreover because ZeroMQ integrates queue management and transport in a monolithic way with special \"\"sockets\"\"', ' etc. More information and instructions for subscribing are at @abstr_hyperlink .  \"', ' evtList){}  Callback function called when user click in a day', ' execute custom python script', ' first columns and then rows are binded to a single tidy dataset.  With the help of dlpyr package data are grouped by subject and activity and then averaged. The result is saved in the working directory as summary.txt  \"', ' for easiest transition from a \"\"Vanilla\"\" server to one enhanced by StarryPy', ' function (err', ' function(err', ' hereby commit to the neveragain.tech pledge. Please stand with me and hold me to it. #neveragaintech\"\" * Post \"\"I', ' homebrew', \" how do the things in parentheses _become_ true or false?  JavaScript lets us compare things. Most of these comparisons come straight from math: we can ask if something is less than something else (enter these in your console!):  @abstr_code_section  We can ask if something is greater than something else:  @abstr_code_section  We can even ask if something is less-than-or-equal-to something else:  @abstr_code_section  or greater-than-or-equal-to something:  @abstr_code_section  How do we test if something is _exactly_ equal to something else? We know that we can't just use =\", ' iOS and Windows). The intelxdk.config.additions.xml file can be used to include options that control your _packaged Cordova web app_ builds. For example', ' if we pass utter nonsense to parseInt()? Go ahead and try it in the console — something like  @abstr_code_section  What did it return? NaN? What is that?  NaN stands for \"\"not a number\"\" — pretty handy', ' including without limitation     the rights to use', ' including without limitation the rights to use', ' including without limitation the rights_ _to use', ' infact due to a logic error in previous code the cookies were not being used anyway. Now Django Select @abstr_number does not use cookies etc.   * Few more bugs fixed in heav_data.js.   * Now production code will use minimized versions of js and css files.   * Codes added in setup.py to automate the task of minimizing js and css files', ' injection and in class annotations', ' is a tuned transport protocol that adds reliability to UDP/IP without sacrificing latency and scalability. A transactioned protocol', ' is much more appropriate for providing reliablity to asynchronous event transport than a streaming protocol.  Morover', ' it can become problematic for high performant systems Furthermore', ' it evaluates the left-hand side of the colon; otherwise it evaluates the right-hand side of the colon.  Syntax:  @abstr_code_section    * Define a function ternaryTeenager that accepts age as a parameter. The body of the function should use the ternary operator to return \"\"You are a teenager\"\" if age is between @abstr_number - @abstr_number and returns \"\"You are not a teenager\"\" if the age is anything else.    Top tip : In order for the function to actually return the evaluation of the ternary operator', ' it is much better suited to many small asynchronous messages and scales better. The drawback of bare UDP/IP is that it is not reliable. What is needed', ' it now has all the functions.  Implementation inheritance is used instead of subtyping to make it easier to understand which functions operate on any given \"\"subject\"\". If you have an element and you need to use a function defined in Node', ' it should return \"\"You are a grownup\"\" Top tip : Remember', ' it will upgrade. - Likewise', ' it\\'s an important skill to have!  \"', ' java', \" jumps to the symbol's declaration. For C/C++/Objective-C\", ' just go to your project folder and run:          github_changelog_generator     * Or', ' just take forever (actually until a timeout is reached). You need to keep this in mind in order to not block your main or UI thread.  Implications of this design choice  Pros:    * A blocking API is much easier to use and understand    Cons:    * You just might accidentally block your UI thread   * You cannot issue thousands of EWS requests asynchronously simply because you cannot spawn thousands of threads in your process. You may need additional effort here    \"', ' libqs.c and the yara signatures except where noted are Copyright @abstr_number Tyler McLellan and Tylabs.  See included Mozilla Public License Version @abstr_number . @abstr_number for licensing information. \"', ' like AMQP', \" loadFeed() is asynchronous so this test wil require the use of Jasmine's beforeEach and asynchronous done() function. @abstr_number . ---Write a test that ensures when a new feed is loaded by the loadFeed function that the content actually changes. Remember\", ' make sure that the instances are being launched with a security group that allows SSH access.  \"', ' memory', ' mitro-core/README) [node', ' network', ' network) in distributed concurrent event driven applications is to use something called micro-threads. A microthread is typically an in-language feature that allows logical concurrency with no more overhead than a function call. Micro threading uses cooperative multi-tasking instead of threads and/or processes and avoids many of the complexities of resource contention', ' no more than the number of cpu cores. This optimizes the use of the cpu power while minimizes the overhead of process context switching.  An example of a framework that uses this type of micro-threaded but multi-process architecture is Erlang. Indeed', ' npm', ' on average across all the days in the dataset', ' on the other hand', ' one might ask', ' one of the best ways to manage and fine tune processor resources (cpu', ' one of the disadvantagesis the inability to manage performance at scale.  A message queuing service performs two distinct but complementary functions.    * The first is asynchronous transport of messages over the internet.   * The second is message queue management', ' or because they are tech geeks. But these data remain under-utilized both because the raw data are hard to obtain and there is a lack of statistical methods and software for processing and interpreting the data.  This assignment makes use of data from a personal activity monitoring device. This device collects data at @abstr_number minute intervals through out the day. The data consists of two months of data from an anonymous individual collected during the months of October and November', ' or both   * Added \"\"returnName\"\" setting which makes the widget return a name instead of HEX value when possible   * Removed many unnecessary features   * Removed dependency on images and made the colour picker completely CSS     \"', ' or if you are usually handling many temporary \"\"to be in a github pull request\"\" branches', ' preface it by @abstr_number . These numbers correspond to the projections in the projectionEnum declaration in elevr-player.js.  If you want to add your picture to the drop-down', ' production)', ' provides increases in security with lower performance requirements relative to over other approaches. LibSodium provides an open source Elliptic Curve Cryptographic library with support for both authentication and encryption. The CurveCP protocol is based on LibSodium and provides a handshake protocol for bootstrapping secure network exchanges of information.  Finally', ' push your changes to your clone and submit a pull request; instructions are available at @abstr_hyperlink . (In case you need them', ' ready-to-use download of the library/framework (unminified) ( @abstr_number )   * Prefer hand-coded/hand-optimized JavaScript over generated/cross-compiled code.   * Running \"\"make\"\" should work and not return an error. To run make', ' requests', ' result will be @abstr_number : @abstr_number', \" rg -uuu is similar to grep -a -r.  @abstr_code_section  (Tip: If your ignore files aren't being adhered to like you expect\", ' rgb()', ' right?  What happens', \" ripgrep defaults to recursive directory search and won't search files ignored by your .gitignore files. It also ignores hidden and binary files by default. ripgrep also implements full support for .gitignore\", ' ruby', \" run the test suite by typing learn and hitting enter. You'll see something similar to:  @abstr_image  You can see your test is currently failing\", ' run this from anywhere:      * github_changelog_generator -u github_username -p github_project     * github_changelog_generator github_username/github_project   * If you are running it against a repository on a Github Enterprise install', ' send an email to  with subject line \"\"[web-animations] ... message topic ...\"\" ( @abstr_hyperlink ).   * For issues with the polyfill', ' simply: \"\"./build.sh test\"\"  To run all the tests  > $ ./build.sh install -DallTests  \"', \" so a typical call to parseInt() looks like  @abstr_code_section  What happens if we pass a representation of a non-integer to parseInt()? Let's try it:  @abstr_code_section  If we enter the above in console\", ' so we leave it out.   * The xact() decorator will set up the connection so that a transaction _is_ started in the relevant block', ' spreadsheet) {         spreadsheet.worksheets[ @abstr_number ].cells({             range: \"\"R @abstr_number C @abstr_number :R @abstr_number C @abstr_number \"\"         }', ' storage', ' such as \"\"m @abstr_number .medium\"\". The default value of this if not specified is \"\"m @abstr_number .medium\"\". \"\"m @abstr_number .small\"\" has been deprecated in \"\"us-east- @abstr_number \"\" and \"\"m @abstr_number .medium\"\" is the smallest instance type to support both paravirtualization and hvm AMIs   * keypair_name - The name of the keypair to use to bootstrap AMIs which support it.   * monitoring - Set to \"\"true\"\" to enable detailed monitoring.   * session_token - The session token provided by STS   * private_ip_address - The private IP address to assign to an instance within a @abstr_hyperlink   * elastic_ip - Can be set to \\'true\\'', ' templates', ' tend to be unreliable under load.  Separating the function of network transport of asynchrounous event from the function of message queue management allows independant tuning at scale of each function.  Most if not all of the MQ services are based on TCP/IP for transport. TCP/IP adds significant latency to the network communications and is therefore not well suited for the asynchronous nature of distibuted event driven application communications. This is primarily due to the way TCP/IP handles connection setup and teardown as well as failed connections in order to support streams. Fundamentally TCP/IP is optomized for sending large contiguous data streams not many small aynchronous events or messages. While not a problem for small scale systems', ' thanks to @abstr_hyperlink ! Tap \"\"Hint\"\" to show hint (e.g. Move left/right/up/down); tap \"\"Auto Run\"\" to run AI automatically. Check it out in the AI branch. You can also check out @abstr_hyperlink .  Thanks to @abstr_hyperlink \\'s Javascript version that gave me (DJBen', ' that is', \" the Erlang ecosystem is somewhat limited in comparison to Python's and the language itself uses what one might describe as a very unfortunate syntax. One of the design objectives behine RAET was to leverage existing Python expertise and the richness of the Python ecosystem but still be able to develop distributed applications using a micro-threaded multi-process architectural model. The goal was to combine the best of both worlds.  RAET is designed to provide secure reliable scalable asynchronous message/event transport over the internet in a micro-threaded multi-process application framework that uses UDP for interhost communication and LibSodium for authentication\", ' the application needs to be able to run at least one process per CPU core. This requires same host inter-process communications. But unlike the conventional approach to multi-processing where there is of one process per logical concurrent function', \" the autoScrolling functionality will still work as expected. The user will also be free to scroll the site with the scroll bar and fullPage.js will fit the section in the screen when scrolling finishes.    * paddingTop: (default @abstr_number) Defines the top padding for each section with a numerical value and its measure (paddingTop: ' @abstr_number px'\", ' the client has little ability to tune the service for performance. Often MQ services become bottlenecks for the distributed application. The more complicated MQ services', ' the completion \"\"getUserAccount\"\" would be ranked higher in the list than the \"\"Fooguxa\"\" completion (both of which are subsequence matches). A word-boundary character are all capital characters', ' the complexities of queue management from the clients. The disadvantage is that at scale', ' the content of the _credentials.py_ file must follow this format:       URL=\"\"https:// @abstr_number . @abstr_number . @abstr_number . @abstr_number \"\"     LOGIN=\"\"admin\"\"     PASSWORD=\"\"Ap @abstr_number cPass @abstr_number \"\"   If the _credentials.py_ does not exist and the credentials are not supplied from the command line', ' the differences in the associated traffic characteristics can become problematic at scale.  Because UDP/IP has lower latency and is connectionless', ' the entire _phrase_) to be true; with ||', ' the identification', ' the link should say \"\" @abstr_number commits\"\".  @abstr_number . You will see a list of commits that you have made to this repository. The most recent commit is at the very top. If this represents the version of the files you want to submit', \" the name of the item you wish to remove. If the item isn't in the cart\", \" the rate limit is only up to @abstr_number requests per hour. Unauthenticated requests are associated with your IP address (not the user making requests).  If you're seeing this warning\", ' the success of the Erlang model provided support for the viability of the RAET approach. Indeed', ' the time when the signal was received', ' the timing of messages', ' then start the dev server as usual with npm start:   \"', ' then the uppercase letters in your query must match uppercase letters in the completion strings (the lowercase letters still match both). So', \" then your     cmake call will be a bit more complicated.  We'll assume you downloaded a     binary distribution of LLVM+Clang from llvm.org in step  @abstr_number  and that you     extracted the archive file to folder ~/ycm_temp/llvm_root_dir (with bin\", ' there is an option of using a specific proxy to extract video information from the site: --extractor-proxy/-y.    \"', ' there will be some results', ' there\\'s nothing you need to do! Ionic @abstr_number projects by default are setup with sass and come with all the build process enabled.  \"', ' therefore', ' this is a little more expensive', ' this mode is based on this function', ' though', ' thus resulting in changing the package provider on Mac OS X', ' tracking', ' type in \"\"ping hostname_A\"\"', ' username and password', ' users will be considered \"\"logged in\"\" if they have an access token stored in their session. So', ' utilizing PWM pins  Layer @abstr_number NUMPAD keys: This is useful for gaming because games can bind separate commands to numpad_ @abstr_number and regular @abstr_number ', ' we intercept all <a href=\"\"/path\"\">...</a> clicks and call msg.setLocation(\"\"/path\"\") for you. If you want to opt out of this', ' we know that our function should look like  @abstr_code_section  But how do we make string all caps? JavaScript has a method for that! It\\'s called toUpperCase(). We can call it on any string:  @abstr_code_section  So let\\'s try it with our shout() function:  @abstr_code_section  And run our tests again:  @abstr_code_section  @abstr_image  Hey! We got one to pass!  \"', ' we need to go over some basic math. In this lab', ' we should have: IP for computer A and B (denoted as IP_A and IP_B later on in the instruction)', \" we'll see that parseInt() forces the parsed number to be an integer — which makes sense when we think about it\", \" we're going to learn about various arithmetic operators. What's an operator\", \" we're going to practice writing functions and manipulating numbers in JavaScript. First\", ' whenever you try to make a Git commit', ' where the volume of messages', ' wherein components are distributed across the internet on multiple hosts and multiple CPU cores', ' which accepts two arguments: the value to parse and the base of the value being parsed. _Usually_ you will want to work with base @abstr_number ', \" which takes our new-found speaking ability to greet our grandmother. She's not exactly deaf\", ' which was created using simulated data :  @abstr_image  Your plot will look different from the one above because you will be using the activity monitor data. Note that the above plot was made using the lattice system but you can make the same version of the plot using any plotting system you choose.  \"', ' why not use Erlang? Unfortunately', ' will be removed in a later version)    \"', ' window', ' you can choose to only include a subset of IntentKit\\'s supported applications. Subspecs exist for each handler class.       # Only includes web browsers     pod \"\"IntentKit/Browsers\"\"   For more information on what subspecs are available', ' you can specify to set the crontab user with:  @abstr_code_section  \"', ' you may find it convenient to run watch-css automatically with npm start', ' you must specify _both_ --github-site and --github-api command line options:          github_changelog_generator --github-site=\"\"https://github.yoursite.com\"\" \\\\                                --github-api=\"\"https://github.yoursite.com/api/v @abstr_number /\"\"      This generates a changelog to the CHANGELOG.md file', ' you must:  @abstr_number ) Create a code library containing one or more functions that receive a vehicle_signal_t and update the appropriate fields of a trip_event_summary_t. * the start time is the time that the first signal of any type is received * the duration is the difference in time between the first signal (of any type) received and the last signal (of any type) received * the distance travelled is the numerical integration of the VEHICLE_SIGNAL_TYPE_VEHICLE_SPEED signal * the total energy consumed is the numerical integration of the product of VEHICLE_SIGNAL_TYPE_HV_BATTERY_VOLTAGE and VEHICLE_SIGNAL_TYPE_HV_BATTERY_CURRENT * the starting SOC is the first VEHICLE_SIGNAL_TYPE_HV_BATTERY_SOC signal received * the ending SOC is the last VEHICLE_SIGNAL_TYPE_HV_BATTERY_SOC signal received  @abstr_number ) Write (at least one) unit test that feeds your library a set of vehicle_signal_t and checks that the resulting trip_event_summary_t is correct. * The included CSV file contains data from one of our employee\\'s vehicles. Use this data to generate a set of vehicle_signal_t for your unit test(s). * A good unit test asserts a result that was derived from an independent source. Document how you determined the \"\"correct value\"\" for each field in trip_event_summary_t. * Note that VEHICLE_SIGNAL_TYPE_HV_BATTERY_CURRENT is signed. An electric vehicle generally discharges the battery (positive current) when driving', ' you say? It\\'s a symbol that _operates_ on one or more (usually two) objects — + is a good example. The + operator says \"\"add what\\'s to the left of + and what\\'s to the right of + together.\"\" Easy-peasy!  As you read through this lesson', ' you should always use the --rebase flag to git pull', \" { \\\\ 'build' : { \\\\ 'mac' : './install.py'\", ' ~/.bash_profile or ~/.zshrc):           export CHANGELOG_GITHUB_TOKEN=\"\"«your- @abstr_number -digit-github-token»\"\"   So', '\"AI  An AI is added', '\"Batching a bunch of messages using the block syntax    require \\'kafka\\'     producer = Kafka::Producer.new     producer.batch do |messages|         puts \"\"Batching a send of multiple messages..\"\"         messages << Kafka::Message.new(\"\"first message to send\"\")         messages << Kafka::Message.new(\"\"second message to send\"\")     end     * they will be sent all at once', '\"Checking Authentication  The first thing we want to do is figure out if a user has already authenticated to Foursquare in this session.  Ultimately', '\"Design Notes  ews-cpp is written in a \"\"modern C++\"\" way:    * C++ Standard Library', '\"How it works  @abstr_number . HyperDev runs the start command which passes server.js through Babel (obeying options in package.json) and then runs the resulting JavaScript. @abstr_number . The server.js defines an Express server and middleware function which passes all requests ending in \"\".js\"\" through Babel\\'s transformFile. @abstr_number . User requests the index in their browser. @abstr_number . Back-end serves index.html. @abstr_number . Browser loads SystemJS. @abstr_number . Browser requests app.js. @abstr_number . Back-end transpiles app.js using same options from step @abstr_number ', '\"Installation  Add this to your composer.json:  JSON { \"\"require\"\": { \"\"thekonz/piximgen\"\": \"\" @abstr_number . @abstr_number .*@dev\"\" } } @abstr_code_section PHP require_once \\'vendor/autoload.php\\'; @abstr_code_section PHP $image = new \\\\thekonz\\\\PixImGen(); @abstr_code_section PHP $image->setSettings([ \\'seed\\' => \\'GitHub rocks!\\' ]); @abstr_code_section PHP header(\\'content-type: image/png\\'); @abstr_code_section PHP echo $image->getImage();    * Look at your image!    @abstr_image  If you play around with the settings (especially the saturation settings)', '\"Installing  This plugin requires KDevelop @abstr_number . @abstr_number .  Just typing cmake . && make && sudo make install should be enough to have the project installed. Then in KDevelop', '\"Introduction  In this lab', '\"Motivation  Modern large scale distributed application architectures', '\"Next steps  Get your computer or device to use the VPN. Please refer to:  Configure IPsec/L @abstr_number TP VPN Clients Configure IPsec/XAuth (\"\"Cisco IPsec\"\") VPN Clients  How-To: IKEv @abstr_number VPN for Windows and Android  If you get an error when trying to connect', '\"Quick Start  @abstr_number . Install dependencies (see browser-ext/README', '\"Rebasing  For feature/topic branches', '\"Road', '\"Sending a sequence of messages    require \\'kafka\\'     producer = Kafka::Producer.new     message @abstr_number  = Kafka::Message.new(\"\"some random message content\"\")     message @abstr_number  = Kafka::Message.new(\"\"some more content\"\")     producer.send([message @abstr_number ', '\"Specifically', '\"Ternary Operator  You can think of it as a shortcut for the if-else statement.  This operator tests a condition; if the condition is truthy', '\"The config.json file  The config.json file currently only requires the key \"\"states_dir\"\" and the value \"\"states/\"\" to tell the test script where to find its states relative to the root of the git repo. The \"\"/\"\" at the end of states is important don\\'t forget it. In the example above', '\"Usage    $ python @abstr_number  aci-fabric-deploy.py --input <spreadhseet>      Optional arguments:       -u URL', '\"Usage  It\\'s really simple!    * If your git remote origin refers to your GitHub repo', '\"Using unit tests  There are various unit tests included in this package to verify if certain modules are operating properly. Simply run the \"\"test\"\" prefixed python script associated with the module you would like to test.  For example to test the DrugDatabase module', '\"We love feedback!   For feedback on the API and the specification:      * File an issue on GitHub:     * Alternatively', '\"href  As a bonus', '\"if-else Statements  You will often see an if statement used in combination with an else clause. An else clause will only get executed if the previous if statement is falsey.  Syntax:  @abstr_code_section    * Define a function teenager that accepts an age as a parameter. If the age is between @abstr_number and @abstr_number it should return \"\"You are a teenager!\"\". Otherwise', '\"parseInt()  The first such tool is the function parseInt()', '否则通过URL自动获取.  @abstr_number . 分享文章给他人'] will be ignored\n  warnings.warn('unknown class(es) {0} will be ignored'\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["##Cross Validate to find the best model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec20073e-3abf-4653-b09d-6407c6af36ac"}}},{"cell_type":"code","source":["def cross_validate_model(classifier, classifier_name):\n  print(f'Running experiment for {classifier_name}')\n  print('Getting per-class scores')\n  y_pred = cross_val_predict(classifier, features_combined.values, labels_matrix, cv=10)\n\n  print('Computing overall results')\n  scores_f1 = cross_val_score(classifier, features_combined.values, labels_matrix, cv=10, scoring='f1_weighted').mean()\n\n  print(classification_report(labels_matrix, y_pred, digits=3))\n  print('f1_weighted : {0}'.format(scores_f1))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bb1d2a6-e84a-45cb-a48b-dd7519ecf4df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##10. Hist Gradient Boosting Classifier"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f835f64b-452c-4a8a-a1f7-ee832bbacdfd"}}},{"cell_type":"code","source":["cross_validate_model(OneVsRestClassifierBalance(HistGradientBoostingClassifier()), 'HistGradientBoostingClassifier()')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69bdf64f-d626-4d83-a45b-5cbae302a29e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Running experiment for HistGradientBoostingClassifier()\nGetting per-class scores\nComputing overall results\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n              precision    recall  f1-score   support\n\n           0      0.479     0.803     0.600       421\n           1      0.555     0.641     0.595       537\n           2      0.747     0.898     0.815      1784\n           3      0.779     0.544     0.641       136\n           4      0.767     0.743     0.755       222\n           5      0.565     0.644     0.602       672\n           6      0.873     0.775     0.821        89\n           7      0.520     0.325     0.400        40\n\n   micro avg      0.653     0.779     0.710      3901\n   macro avg      0.661     0.672     0.654      3901\nweighted avg      0.663     0.779     0.711      3901\n samples avg      0.683     0.761     0.701      3901\n\nf1_weighted : nan\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Running experiment for HistGradientBoostingClassifier()\nGetting per-class scores\nComputing overall results\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3298, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n              precision    recall  f1-score   support\n\n           0      0.479     0.803     0.600       421\n           1      0.555     0.641     0.595       537\n           2      0.747     0.898     0.815      1784\n           3      0.779     0.544     0.641       136\n           4      0.767     0.743     0.755       222\n           5      0.565     0.644     0.602       672\n           6      0.873     0.775     0.821        89\n           7      0.520     0.325     0.400        40\n\n   micro avg      0.653     0.779     0.710      3901\n   macro avg      0.661     0.672     0.654      3901\nweighted avg      0.663     0.779     0.711      3901\n samples avg      0.683     0.761     0.701      3901\n\nf1_weighted : nan\n/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 202, in fit\n    X, y = self._validate_data(X, y, dtype=[X_DTYPE],\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/base.py\", line 433, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 826, in check_X_y\n    y = column_or_1d(y, warn=True)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 864, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (3299, 8) instead.\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF612 Final Project with Histogram Gradient Boost - Old Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4424314020178505}},"nbformat":4,"nbformat_minor":0}
